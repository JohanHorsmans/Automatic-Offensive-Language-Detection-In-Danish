---
title: "Automatic Offensive Language Detection In Danish"
author: Aske Bredahl & Johan Horsmans
output: github_document
---

```{r setup}
#Loading required packages

library(pacman)
p_load(readr, tidyverse,rsample,recipes, textrecipes, parsnip, yardstick,workflows, discrim,kernlab,stringr, tm, ggplot2, GGally, e1071, caret,stopwords, stringi, SnowballC,fastmatch, parsnip, keras)
```

```{r reading data and preprocessing}

loading_data <- function(path) {
  read_delim(path, "\t", escape_double = FALSE, trim_ws = TRUE)
}

corpus <- loading_data("offenseval-da-training-v1.tsv") %>% 
          mutate(Id = id,label = factor(subtask_a),text=tweet) %>% 
          na.omit()
testing<-loading_data("offenseval-da-test-v1.tsv") %>% 
          mutate(Id = id,label = factor(subtask_a),text=tweet) %>% 
          na.omit()


###CLEANING####

# Remove stopwords
testing$text<-tolower(testing$text) 
corpus$text<-tolower(corpus$text) 

stopwords_regex = paste(stopwords("da",source= "snowball"), collapse = '\\b|\\b')
stopwords_regex = paste0('\\b', stopwords_regex, '\\b')
corpus$text = stringr::str_replace_all(corpus$text, stopwords_regex, '')
# remove numbers
corpus$text <-  removeNumbers(corpus$text)
# Stem words
corpus$text <-  wordStem(corpus$text, language = "danish")
#repeat for test data
stopwords_regex = paste(stopwords("da",source= "snowball"), collapse = '\\b|\\b')
stopwords_regex = paste0('\\b', stopwords_regex, '\\b')
testing$text = stringr::str_replace_all(testing$text, stopwords_regex, '')
# remove numbers
testing$text <-  removeNumbers(testing$text)
# Stem words
testing$text <-  wordStem(testing$text, language = "danish")
# remove punctuation
testing$text<-removePunctuation(testing$text)
corpus$text<-removePunctuation(corpus$text)


###CLEANING###


corpus<-corpus[,4:6]

testing<-testing[,4:6]

training_set <- corpus
test_set <- testing
#


# General toxen recipe
text_recipe <- recipe(label ~ ., data = training_set) %>% 
  update_role(Id, new_role = "ID") %>% 
  step_tokenize(text, engine = "spacyr", token = "words") %>%
 ## step_stopwords(text) %>% 
  step_lemma(text) %>%
  step_tokenfilter(text, max_tokens = 100) %>%
  step_tfidf(text)
#

# NGRAM recipie
text_recipe <- recipe(label ~ ., data = training_set) %>% 
  update_role(Id, new_role = "ID") %>% 
  step_tokenize(text) %>%
  step_ngram(text, num_tokens = 5) %>%
 ## step_stopwords(text) %>% 
  ##step_lemma(text) %>%
  step_tokenfilter(text, max_tokens = 100) %>%
  step_tfidf(text)

#TRI GRAM:
#rec <- recipe(~ text, data = abc_tibble) %>%
 # step_tokenize(text) %>%
  #step_ngram(text, num_tokens = 3) %>%
  #step_tokenfilter(text) %>%
  #step_tf(text)

```

```{r}
text_model_log_spec <- logistic_reg() %>% set_engine("glm") %>% set_mode("classification")
text_model_NB_spec <- naive_Bayes() %>% set_engine("naivebayes") %>% set_mode("classification")
text_model_svm_spec <- svm_poly("classification") %>% set_engine("kernlab")
```

```{r}
text_model_log_wf <- workflows::workflow() %>% add_recipe(text_recipe) %>% add_model(text_model_log_spec)
text_model_NB_wf <- workflows::workflow() %>% add_recipe(text_recipe) %>% add_model(text_model_NB_spec)
text_model_svm_wf <- workflows::workflow() %>% add_recipe(text_recipe) %>% add_model(text_model_svm_spec)

```

```{r}
fit_log_model <- fit(text_model_log_wf, training_set)
fit_NB_model <- fit(text_model_NB_wf, training_set)
fit_svm_model <- fit(text_model_svm_wf, training_set)

```

```{r}
predictions_log <- predict(fit_log_model, test_set)
predictions_log$raw_log <- predict(fit_log_model, test_set,type="prob")

predictions_NB <- predict(fit_NB_model, test_set,type="class")
predictions_NB$raw_NB <- predict(fit_NB_model, test_set,type="prob")

predictions_SVM <- stats::predict(fit_svm_model, test_set,type="class")
predictions_SVM$raw_svm <- stats::predict(fit_svm_model, test_set,type="prob")



#testing$predictions<-predictions_NB


#Evaluate
bind_cols(test_set,predictions_log) %>% conf_mat(label, .pred_class) 
bind_cols(test_set,predictions_log) %>% accuracy(truth = label, estimate = .pred_class)

bind_cols(test_set,predictions_NB) %>% conf_mat(label, .pred_class) 
bind_cols(test_set,predictions_NB) %>% accuracy(truth = label, estimate = .pred_class)

bind_cols(test_set,predictions_SVM) %>% conf_mat(label, .pred_class) 
bind_cols(test_set,predictions_SVM) %>% accuracy(truth = label, estimate = .pred_class)


```

Ensemble averaging pipeline:
```{r}
#Load BERT data:
Bert_results<-read_csv("OG_BERT_RESULTS.csv")
#testing$BERT_not_prob<-Bert_results$`0`
#testing$BERT_off_prob<-Bert_results$`1`

#Create ensemble with raw probs of other models:
Ensemble_probabilities<-predictions_SVM$raw_svm
Ensemble_probabilities$SVM<-predictions_SVM$raw_svm
Ensemble_probabilities<-Ensemble_probabilities[,3]
Ensemble_probabilities$NB<-predictions_NB$raw_NB
Ensemble_probabilities$log<-predictions_log$raw_log


#Define empty columns in testing set
Ensemble_probabilities$ensemble_plus_bert_probs_off<-1
Ensemble_probabilities$ensemble_plus_bert_probs_not<-1
Ensemble_probabilities$avg_preds_bert_plus_ensemble<-1
testing$support_system<-1
Ensemble_probabilities$bertpreds<-1
Ensemble_probabilities$bert_off<-Bert_results$`1`
Ensemble_probabilities$bert_not<-Bert_results$`0`


#Calculating average OFF prob and class prediction
#for (i in 1:nrow(Ensemble_probabilities)){
#  Ensemble_probabilities$ensemble_probs_off[i] <- (Ensemble_probabilities$SVM$.pred_OFF[i] + Ensemble_probabilities$NB$.pred_OFF[i] + Ensemble_probabilities$log$.pred_OFF[i]) / 3
#}

#Calculating average NOT prob
#for (i in 1:nrow(Ensemble_probabilities)){
#  testing$ensemble_probs_not[i] <- (Ensemble_probabilities$SVM$.pred_NOT[i] + Ensemble_probabilities$NB$.pred_NOT[i] + Ensemble_probabilities$log$.pred_NOT[i]) / 3
#  testing$avg_preds[i] <- ifelse(testing$ensemble_probs_off[i] > 0.5, 1, 0)
#  }


#Calculating average OFF BERT+Ensemble prob
for (i in 1:nrow(Ensemble_probabilities)){
  Ensemble_probabilities$ensemble_plus_bert_probs_off[i] <- (Ensemble_probabilities$SVM$.pred_OFF[i] + Ensemble_probabilities$NB$.pred_OFF[i] + Ensemble_probabilities$log$.pred_OFF[i] + Bert_results$`1`[i]) / 4
}

#Calculating average NOT BERT+Ensemble prob and making binary classification
for (i in 1:nrow(Ensemble_probabilities)){
  Ensemble_probabilities$ensemble_plus_bert_probs_not[i] <- (Ensemble_probabilities$SVM$.pred_NOT[i] + Ensemble_probabilities$NB$.pred_NOT[i] + Ensemble_probabilities$log$.pred_NOT[i] + Bert_results$`0`[i]) / 4
  Ensemble_probabilities$avg_preds_bert_plus_ensemble[i] <- ifelse(Ensemble_probabilities$ensemble_plus_bert_probs_off[i] > 0.5, 1, 0)
}

#Calculate binary BERT classification
for (i in 1:nrow(Ensemble_probabilities)){
  Ensemble_probabilities$bertpreds[i] <- ifelse(Ensemble_probabilities$bert_off[i] > 0.5, 1, 0)
}

##ENSEMBLE SUPPORT SYSTEM##

# Making support system integrating "unsure" offensive classifications (by BERT)
for (i in 1:nrow(testing)){
  testing$support_system[i]<-ifelse(Ensemble_probabilities$bert_off[i] > 0.5 & Ensemble_probabilities$bert_off[i] < 0.65, Ensemble_probabilities$avg_preds_bert_plus_ensemble[i], Ensemble_probabilities$bertpreds[i])
}

# Same but for NOT
for (i in 1:nrow(testing)){
  testing$support_system[i]<-ifelse(Ensemble_probabilities$bert_not[i] > 0.5 & Ensemble_probabilities$bert_not[i] < 0.65, Ensemble_probabilities$avg_preds_bert_plus_ensemble[i], Ensemble_probabilities$bertpreds[i])
}

(Ensemble_probabilities$SVM$.pred_NOT[151]+Ensemble_probabilities$NB$.pred_NOT[151]+Ensemble_probabilities$log$.pred_NOT[151])/3

(Ensemble_probabilities$SVM$.pred_OFF[151]+Ensemble_probabilities$NB$.pred_OFF[151]+Ensemble_probabilities$log$.pred_OFF[151])/3

```


##TEST only with NB (to see if 151 changes)

```{r}
#Load BERT data:
Bert_results<-read_csv("OG_BERT_RESULTS.csv")
#testing$BERT_not_prob<-Bert_results$`0`
#testing$BERT_off_prob<-Bert_results$`1`

#Create ensemble with raw probs of other models:
Ensemble_probabilities<-predictions_NB$raw_NB
Ensemble_probabilities$SVM<-predictions_SVM$raw_svm
Ensemble_probabilities<-Ensemble_probabilities[,3]
Ensemble_probabilities$NB<-predictions_NB$raw_NB
Ensemble_probabilities$log<-predictions_log$raw_log


#Define empty columns in testing set
Ensemble_probabilities$ensemble_plus_bert_probs_off<-1
Ensemble_probabilities$ensemble_plus_bert_probs_not<-1
Ensemble_probabilities$avg_preds_bert_plus_ensemble<-1
testing$support_system<-1
Ensemble_probabilities$bertpreds<-1
Ensemble_probabilities$bert_off<-Bert_results$`1`
Ensemble_probabilities$bert_not<-Bert_results$`0`


#Calculating average OFF prob and class prediction
#for (i in 1:nrow(Ensemble_probabilities)){
#  Ensemble_probabilities$ensemble_probs_off[i] <- (Ensemble_probabilities$SVM$.pred_OFF[i] + Ensemble_probabilities$NB$.pred_OFF[i] + Ensemble_probabilities$log$.pred_OFF[i]) / 3
#}

#Calculating average NOT prob
#for (i in 1:nrow(Ensemble_probabilities)){
#  testing$ensemble_probs_not[i] <- (Ensemble_probabilities$SVM$.pred_NOT[i] + Ensemble_probabilities$NB$.pred_NOT[i] + Ensemble_probabilities$log$.pred_NOT[i]) / 3
#  testing$avg_preds[i] <- ifelse(testing$ensemble_probs_off[i] > 0.5, 1, 0)
#  }


#Calculating average OFF BERT+Ensemble prob
for (i in 1:nrow(Ensemble_probabilities)){
  Ensemble_probabilities$ensemble_plus_bert_probs_off[i] <- (Ensemble_probabilities$NB$.pred_OFF[i] + Bert_results$`1`[i]) / 2
}

#Calculating average NOT BERT+Ensemble prob and making binary classification
for (i in 1:nrow(Ensemble_probabilities)){
  Ensemble_probabilities$ensemble_plus_bert_probs_not[i] <- (Ensemble_probabilities$NB$.pred_NOT[i] + Bert_results$`0`[i]) / 2
  Ensemble_probabilities$avg_preds_bert_plus_ensemble[i] <- ifelse(Ensemble_probabilities$ensemble_plus_bert_probs_off[i] > 0.5, 1, 0)
}

#Calculate binary BERT classification
for (i in 1:nrow(Ensemble_probabilities)){
  Ensemble_probabilities$bertpreds[i] <- ifelse(Ensemble_probabilities$bert_off[i] > 0.5, 1, 0)
}

##ENSEMBLE SUPPORT SYSTEM##

# Making support system integrating "unsure" offensive classifications (by BERT)
for (i in 1:nrow(testing)){
  testing$support_system[i]<-ifelse(Ensemble_probabilities$bert_off[i] > 0.5 & Ensemble_probabilities$bert_off[i] < 0.65, Ensemble_probabilities$avg_preds_bert_plus_ensemble[i], Ensemble_probabilities$bertpreds[i])
}

# Same but for NOT
for (i in 1:nrow(testing)){
  testing$support_system[i]<-ifelse(Ensemble_probabilities$bert_not[i] > 0.5 & Ensemble_probabilities$bert_not[i] < 0.65, Ensemble_probabilities$avg_preds_bert_plus_ensemble[i], Ensemble_probabilities$bertpreds[i])
}

(Ensemble_probabilities$SVM$.pred_NOT[151]+Ensemble_probabilities$NB$.pred_NOT[151]+Ensemble_probabilities$log$.pred_NOT[151])/3

(Ensemble_probabilities$SVM$.pred_OFF[151]+Ensemble_probabilities$NB$.pred_OFF[151]+Ensemble_probabilities$log$.pred_OFF[151])/3
```

```{r}
loading_data <- function(path) {
  read_delim(path, "\t", escape_double = FALSE, trim_ws = TRUE)
}

training <- loading_data("offenseval-da-training-v1.tsv") %>% 
          mutate(Id = id,tag = factor(subtask_a),text=tweet) %>% 
          na.omit()

testing<-loading_data("offenseval-da-test-v1.tsv") %>% 
          mutate(Id = id,tag = factor(subtask_a),text=tweet) %>% 
          na.omit()


###CLEANING####
# Remove stopwords
testing$text<-tolower(testing$text) 
training$text<-tolower(training$text) 
stopwords_regex = paste(stopwords("da",source= "snowball"), collapse = '\\b|\\b')
stopwords_regex = paste0('\\b', stopwords_regex, '\\b')
training$text = stringr::str_replace_all(training$text, stopwords_regex, '')
# remove numbers
training$text <-  removeNumbers(training$text)
# Stem words
training$text <-  wordStem(training$text, language = "danish")
#repeat for test data
stopwords_regex = paste(stopwords("da",source= "snowball"), collapse = '\\b|\\b')
stopwords_regex = paste0('\\b', stopwords_regex, '\\b')
testing$text = stringr::str_replace_all(testing$text, stopwords_regex, '')
# remove numbers
testing$text <-  removeNumbers(testing$text)
# Stem words
testing$text <-  wordStem(testing$text, language = "danish")
# remove punctuation
testing$text<-removePunctuation(testing$text)
training$text<-removePunctuation(training$text)


training<-training[,4:6]
testing<-testing[,4:6]

training$tag<-as.numeric(training$tag)-1
testing$tag<-as.numeric(testing$tag)-1

glimpse(training)


###

```

```{r}
#train

text <- training$text

max_features <- 1000
tokenizer <- text_tokenizer(num_words = max_features)

#test

text_test <- testing$text

max_features_test <- 1000
tokenizer_test <- text_tokenizer(num_words = max_features)
```

```{r}
#Train
tokenizer %>% 
  fit_text_tokenizer(text)

#Test
tokenizer_test %>% 
  fit_text_tokenizer(text_test)
```

```{r}
tokenizer$document_count
tokenizer_test$document_count

```

```{r}
tokenizer$word_index %>%
  head()

tokenizer_test$word_index %>%
  head()

```

```{r}
text_seqs <- texts_to_sequences(tokenizer, text)

text_seqs %>%
  head()

#Test
text_seqs_test <- texts_to_sequences(tokenizer_test, text_test)

text_seqs_test %>%
  head()
```


```{r}
##
maxlen <- 100
batch_size <- 32
embedding_dims <- 50
filters <- 64
kernel_size <- 3
hidden_dims <- 50
epochs <- 5

```

```{r}
x_train <- text_seqs %>%
  pad_sequences(maxlen = maxlen)
dim(x_train)

#Test
x_test <- text_seqs_test %>%
  pad_sequences(maxlen = maxlen)
dim(x_test)
```

```{r}
y_train <- training$tag
length(y_train)

#Test
y_test <- testing$tag
length(y_test)

y_test<-y_test
y_train<-y_train
```

##LSTM

```{r}
model <- keras_model_sequential()
model %>%
  layer_embedding(input_dim = max_features, output_dim = 128) %>% 
  layer_lstm(units = 64, dropout = 0.2, recurrent_dropout = 0.2) %>% 
  layer_dense(units = 1, activation = 'sigmoid')

```

```{r}
# Try using different optimizers and different optimizer configs
model %>% compile(
  loss = 'binary_crossentropy',
  optimizer = 'adam',
  metrics = c('accuracy')
)
```

```{r}

model %>% fit(
  x_train, y_train,
  batch_size = batch_size,
  epochs = 15,
  validation_data = list(x_test, y_test)

)
```

```{r}
scores <- model %>% tensorflow::evaluate(
  x_test, y_test,
  batch_size = batch_size
)



y_test
x_test

#testing$predict<-predict_classes(model, testing$tag)



testing$predict<-predict_classes(model, x_test) #Predict class
Raw_probs<-predict_proba(model, x_test) #Predict raw probabilites (prob of tweet = OFF)

testing$raw_probs_off<-predict_proba(model, x_test)
testing$raw_probs_not<-1-testing$raw_probs_off

#testing<-testing[,1:3]
```

#CNN:

```{r}
# Embedding
max_features = 20000
maxlen = 100
embedding_size = 128

#2nd run
max_features = 20000
maxlen = 100
embedding_size = 128
```

```{r}
# Convolution
kernel_size = 5
filters = 64
pool_size = 4

#2nd run
kernel_size = 2
filters = 32
pool_size = 4
```

```{r}
# LSTM
lstm_output_size = 70

#2nd run

lstm_output_size = 70

```

```{r}
# Training
batch_size = 30
epochs = 15

#2nd run
batch_size = 15
epochs = 5
```

```{r}
# Defining Model ------------------------------------------------------

model <- keras_model_sequential()

model %>%
  layer_embedding(max_features, embedding_size, input_length = maxlen) %>%
  layer_dropout(0.25) %>%
  layer_conv_1d(
    filters, 
    kernel_size, 
    padding = "valid",
    activation = "relu",
    strides = 1
  ) %>%
  layer_max_pooling_1d(pool_size) %>%
  layer_lstm(lstm_output_size) %>%
  layer_dense(1) %>%
  layer_activation("sigmoid")

#Set learning rate (default 0.001)
optimizer_adam(
  lr = 0.001)

model %>% compile(
  loss = "binary_crossentropy",
  optimizer = "adam",
  metrics = c("accuracy","Precision","Recall")
)
```

```{r}
model %>% fit(
  x_train, y_train,
  batch_size = batch_size,
  epochs = epochs,
  validation_data = list(x_test, y_test)
)

scores <- model %>% tensorflow::evaluate(
  x_test, y_test,
  batch_size = batch_size
)

testing$predict<-predict_classes(model, x_test) #Predict class
Raw_probs<-predict_proba(model, x_test) #Predict raw probabilites (prob of tweet = OFF)



```

#Bi-directional LSTM

```{r}
# Define maximum number of input features
max_features <- 20000
```

```{r}
# Cut texts after this number of words
# (among top max_features most common words)
maxlen <- 100
```

```{r}
batch_size <- 32

```

```{r}
#Initialize model
model <- keras_model_sequential()
model %>%
  # Creates dense embedding layer; outputs 3D tensor
  # with shape (batch_size, sequence_length, output_dim)
  layer_embedding(input_dim = max_features, 
                  output_dim = 128, 
                  input_length = maxlen) %>% 
  bidirectional(layer_lstm(units = 64)) %>%
  layer_dropout(rate = 0.5) %>% 
  layer_dense(units = 1, activation = 'sigmoid')

```

```{r}
# Try using different optimizers and different optimizer configs
model %>% compile(
  loss = 'binary_crossentropy',
  optimizer = 'adam',
  metrics = c('accuracy',"Recall","Precision")
)

#Change learning rate when something does not improve
callback_reduce_lr_on_plateau(
  monitor = "",
  factor = 0.1, #How much to reduce LR
  patience = 50, #How many epocs wiht no improvement
  verbose = 0,
  mode = c("auto", "min", "max"),
  min_delta = 1e-04,
  cooldown = 0,
  min_lr = 0
)
```

```{r}
# Train model over four epochs
model %>% fit(
  x_train, y_train,
  batch_size = batch_size,
  epochs = 15,
  validation_data = list(x_test, y_test)
)

testing$predict<-predict_classes(model, x_test) #Predict class
Raw_probs<-predict_proba(model, x_test) #Predict raw probabilites (prob of tweet = OFF)


```




